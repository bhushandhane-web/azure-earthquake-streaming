{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86fa205d-233c-4994-9af7-70d1a6999faf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Switch to your new catalog\n",
    "USE CATALOG databricks_cata;\n",
    "\n",
    "-- Create the 'default' schema if it's missing\n",
    "CREATE SCHEMA IF NOT EXISTS bronze;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be7b0827-935f-4a24-9e3e-145baa87f94e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 1. CONFIGURATION\n",
    "# ==========================================\n",
    "storage_account_name = \"databricksete1995\"\n",
    "\n",
    "# PATHS\n",
    "# Input: Where Event Hubs dumps the raw Avro files\n",
    "source_path = f\"abfss://source@{storage_account_name}.dfs.core.windows.net/eh-tsunami-project/earthquake-stream\"\n",
    "\n",
    "# Output: Where you want the clean Delta Table to live\n",
    "bronze_path = f\"abfss://bronze@{storage_account_name}.dfs.core.windows.net/earthquake_raw\"\n",
    "\n",
    "# System Paths (Checkpoints track what has been processed)\n",
    "# It is best practice to keep checkpoints inside the target container but hidden\n",
    "checkpoint_path = f\"abfss://bronze@{storage_account_name}.dfs.core.windows.net/_checkpoints/earthquake_load\"\n",
    "schema_path     = f\"abfss://bronze@{storage_account_name}.dfs.core.windows.net/_checkpoints/earthquake_schema\"\n",
    "\n",
    "table_name = \"databricks_cata.bronze.bronze_earthquake\"\n",
    "\n",
    "print(f\"ðŸ“¥ Reading from: {source_path}\")\n",
    "print(f\"ðŸ“¤ Writing to:   {bronze_path}\")\n",
    "\n",
    "# ==========================================\n",
    "# 2. READ STREAM (From Source Container)\n",
    "# ==========================================\n",
    "df_raw = (spark.readStream\n",
    "    .format(\"cloudFiles\")               # Use Auto Loader\n",
    "    .option(\"cloudFiles.format\", \"avro\")# Read Avro format\n",
    "    .option(\"cloudFiles.schemaLocation\", schema_path)\n",
    "    .load(source_path))\n",
    "\n",
    "# ==========================================\n",
    "# 3. TRANSFORM (Make it Readable)\n",
    "# ==========================================\n",
    "# Event Hubs Avro wraps the real data in a binary 'Body' field.\n",
    "# We cast it to STRING to make it readable JSON.\n",
    "df_readable = df_raw.selectExpr(\n",
    "    \"cast(Body as string) as json_payload\", # <--- This makes it readable\n",
    "    \"EnqueuedTimeUtc as ingestion_time\",\n",
    "    \"SequenceNumber\",\n",
    "    \"Offset\"\n",
    ")\n",
    "\n",
    "# ==========================================\n",
    "# 4. WRITE STREAM (To Bronze Container)\n",
    "# ==========================================\n",
    "query = (df_readable.writeStream\n",
    "    .format(\"delta\")                    # Write as Delta Lake (Parquet-based)\n",
    "    .outputMode(\"append\")\n",
    "    .option(\"checkpointLocation\", checkpoint_path)\n",
    "    .option(\"path\", bronze_path)        # <--- Forces storage in 'bronze' container\n",
    "    .option(\"mergeSchema\", \"true\")      # Auto-adjust if columns change\n",
    "    .trigger(availableNow=True)\n",
    "    .table(table_name))                 # Registers table in Databricks\n",
    "\n",
    "print(f\"ðŸš€ Pipeline started! Converting Avro to Delta...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4dd23ca9-6b9a-4be2-bd35-2a389eb63f0f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load the table into a DataFrame\n",
    "df = spark.table(\"databricks_cata.bronze.bronze_earthquake\")\n",
    "\n",
    "# Display the first few rows\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ddf23774-7788-489a-9bd7-db944e28cf35",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE TEMP VIEW v_earthquake_complete AS\n",
    "SELECT \n",
    "    -- ==========================================\n",
    "    -- 1. IDENTIFIERS & GEOMETRY\n",
    "    -- ==========================================\n",
    "    get_json_object(json_payload, '$.id') as id,\n",
    "    get_json_object(json_payload, '$.type') as feature_type,\n",
    "    \n",
    "    -- Geometry (Coordinates: [Longitude, Latitude, Depth])\n",
    "    CAST(get_json_object(json_payload, '$.geometry.coordinates[0]') AS DECIMAL(10,4)) as longitude,\n",
    "    CAST(get_json_object(json_payload, '$.geometry.coordinates[1]') AS DECIMAL(10,4)) as latitude,\n",
    "    CAST(get_json_object(json_payload, '$.geometry.coordinates[2]') AS DECIMAL(10,2)) as depth,\n",
    "\n",
    "    -- ==========================================\n",
    "    -- 2. MAIN PROPERTIES\n",
    "    -- ==========================================\n",
    "    CAST(get_json_object(json_payload, '$.properties.mag') AS DECIMAL(4,2)) as mag,\n",
    "    get_json_object(json_payload, '$.properties.magType') as magType,\n",
    "    get_json_object(json_payload, '$.properties.place') as place,\n",
    "    get_json_object(json_payload, '$.properties.type') as type, -- e.g., \"earthquake\"\n",
    "\n",
    "    -- Time (Converted from Epoch Long Integer to Timestamp)\n",
    "    CAST(get_json_object(json_payload, '$.properties.time') / 1000 AS TIMESTAMP) as time_utc,\n",
    "    CAST(get_json_object(json_payload, '$.properties.updated') / 1000 AS TIMESTAMP) as updated_utc,\n",
    "    CAST(get_json_object(json_payload, '$.properties.tz') AS INTEGER) as tz,\n",
    "\n",
    "    -- ==========================================\n",
    "    -- 3. ALERTS & INTENSITY\n",
    "    -- ==========================================\n",
    "    get_json_object(json_payload, '$.properties.alert') as alert, -- e.g., \"green\", \"red\"\n",
    "    get_json_object(json_payload, '$.properties.status') as status, -- e.g., \"automatic\", \"reviewed\"\n",
    "    CAST(get_json_object(json_payload, '$.properties.tsunami') AS INTEGER) as tsunami,\n",
    "    CAST(get_json_object(json_payload, '$.properties.sig') AS INTEGER) as sig, -- Significance\n",
    "    CAST(get_json_object(json_payload, '$.properties.felt') AS INTEGER) as felt, -- Number of reports\n",
    "    CAST(get_json_object(json_payload, '$.properties.cdi') AS DECIMAL(4,1)) as cdi, -- Community intensity\n",
    "    CAST(get_json_object(json_payload, '$.properties.mmi') AS DECIMAL(4,2)) as mmi, -- Instrumental intensity\n",
    "\n",
    "    -- ==========================================\n",
    "    -- 4. TECHNICAL / SENSOR DATA\n",
    "    -- ==========================================\n",
    "    get_json_object(json_payload, '$.properties.net') as net, -- Network (e.g., \"us\", \"ak\")\n",
    "    get_json_object(json_payload, '$.properties.code') as code,\n",
    "    get_json_object(json_payload, '$.properties.ids') as ids,\n",
    "    get_json_object(json_payload, '$.properties.sources') as sources,\n",
    "    get_json_object(json_payload, '$.properties.types') as types,\n",
    "    \n",
    "    CAST(get_json_object(json_payload, '$.properties.nst') AS INTEGER) as nst, -- Number of Stations\n",
    "    CAST(get_json_object(json_payload, '$.properties.dmin') AS DECIMAL(10,4)) as dmin, -- Min Distance\n",
    "    CAST(get_json_object(json_payload, '$.properties.rms') AS DECIMAL(10,4)) as rms, -- Root Mean Square\n",
    "    CAST(get_json_object(json_payload, '$.properties.gap') AS DECIMAL(10,2)) as gap, -- Azimuthal Gap\n",
    "\n",
    "    -- ==========================================\n",
    "    -- 5. URLS\n",
    "    -- ==========================================\n",
    "    get_json_object(json_payload, '$.properties.url') as url,\n",
    "    get_json_object(json_payload, '$.properties.detail') as detail\n",
    "\n",
    "FROM databricks_cata.bronze.bronze_earthquake;\n",
    "\n",
    "-- Verify the output\n",
    "SELECT * FROM v_earthquake_complete ORDER BY time_utc DESC LIMIT 10;"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5326107814166628,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "(Clone) Bronze_Layer",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
