{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6671bbe6-733b-4ade-8a75-0a3996e1c03b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import from_json, col, to_timestamp\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, LongType, IntegerType, ArrayType, DecimalType\n",
    "\n",
    "# ==========================================\n",
    "# 1. CONFIGURATION\n",
    "# ==========================================\n",
    "storage_account_name = \"databricksete1995\"\n",
    "silver_container     = \"silver\"\n",
    "\n",
    "# Paths\n",
    "silver_path     = f\"abfss://{silver_container}@{storage_account_name}.dfs.core.windows.net/earthquake_clean\"\n",
    "checkpoint_path = f\"abfss://{silver_container}@{storage_account_name}.dfs.core.windows.net/_checkpoints/earthquake_silver\"\n",
    "\n",
    "# Table Names (Unity Catalog)\n",
    "source_table = \"databricks_cata.bronze.bronze_earthquake\"\n",
    "target_table = \"databricks_cata.silver.silver_earthquake\"\n",
    "\n",
    "# ==========================================\n",
    "# 2. DEFINE SCHEMA (Matches your Producer Data)\n",
    "# ==========================================\n",
    "# This tells Spark exactly how to read the JSON inside 'json_payload'\n",
    "json_schema = StructType([\n",
    "    StructField(\"id\", StringType(), True),\n",
    "    StructField(\"type\", StringType(), True),\n",
    "    StructField(\"geometry\", StructType([\n",
    "        StructField(\"coordinates\", ArrayType(DoubleType()), True) # [Lon, Lat, Depth]\n",
    "    ])),\n",
    "    StructField(\"properties\", StructType([\n",
    "        StructField(\"mag\", DoubleType(), True),\n",
    "        StructField(\"place\", StringType(), True),\n",
    "        StructField(\"time\", LongType(), True),       # Unix MS\n",
    "        StructField(\"updated\", LongType(), True),    # Unix MS\n",
    "        StructField(\"tz\", IntegerType(), True),\n",
    "        StructField(\"url\", StringType(), True),\n",
    "        StructField(\"detail\", StringType(), True),\n",
    "        StructField(\"felt\", IntegerType(), True),\n",
    "        StructField(\"cdi\", DoubleType(), True),\n",
    "        StructField(\"mmi\", DoubleType(), True),\n",
    "        StructField(\"alert\", StringType(), True),\n",
    "        StructField(\"status\", StringType(), True),\n",
    "        StructField(\"tsunami\", IntegerType(), True),\n",
    "        StructField(\"sig\", IntegerType(), True),\n",
    "        StructField(\"net\", StringType(), True),\n",
    "        StructField(\"code\", StringType(), True),\n",
    "        StructField(\"ids\", StringType(), True),\n",
    "        StructField(\"sources\", StringType(), True),\n",
    "        StructField(\"types\", StringType(), True),\n",
    "        StructField(\"nst\", IntegerType(), True),\n",
    "        StructField(\"dmin\", DoubleType(), True),\n",
    "        StructField(\"rms\", DoubleType(), True),\n",
    "        StructField(\"gap\", DoubleType(), True),\n",
    "        StructField(\"magType\", StringType(), True),\n",
    "        StructField(\"type\", StringType(), True)\n",
    "    ]))\n",
    "])\n",
    "\n",
    "print(f\"ðŸš€ Silver Stream Initializing...\")\n",
    "print(f\"ðŸ“¥ Reading from: {source_table}\")\n",
    "print(f\"ðŸ“¤ Writing to:   {target_table}\")\n",
    "\n",
    "# ==========================================\n",
    "# 3. READ STREAM (From Bronze Delta Table)\n",
    "# ==========================================\n",
    "df_bronze = spark.readStream.table(source_table)\n",
    "\n",
    "# ==========================================\n",
    "# 4. TRANSFORMATION (Parse & Flatten)\n",
    "# ==========================================\n",
    "df_parsed = df_bronze.withColumn(\"parsed\", from_json(col(\"json_payload\"), json_schema))\n",
    "\n",
    "df_silver = df_parsed.select(\n",
    "    # Keep Metadata\n",
    "    col(\"ingestion_time\"),\n",
    "    \n",
    "    # 1. Identity\n",
    "    col(\"parsed.id\").alias(\"event_id\"),\n",
    "    col(\"parsed.type\").alias(\"feature_type\"),\n",
    "    \n",
    "    # 2. Geometry (Split Array into Columns)\n",
    "    col(\"parsed.geometry.coordinates\")[0].alias(\"longitude\"),\n",
    "    col(\"parsed.geometry.coordinates\")[1].alias(\"latitude\"),\n",
    "    col(\"parsed.geometry.coordinates\")[2].alias(\"depth\"),\n",
    "    \n",
    "    # 3. Properties (Main)\n",
    "    col(\"parsed.properties.mag\").alias(\"magnitude\"),\n",
    "    col(\"parsed.properties.magType\").alias(\"magnitude_type\"),\n",
    "    col(\"parsed.properties.place\").alias(\"place\"),\n",
    "    \n",
    "    # Time Conversion (Unix MS -> Timestamp)\n",
    "    (col(\"parsed.properties.time\") / 1000).cast(\"timestamp\").alias(\"event_time\"),\n",
    "    (col(\"parsed.properties.updated\") / 1000).cast(\"timestamp\").alias(\"updated_time\"),\n",
    "    col(\"parsed.properties.tz\").alias(\"timezone_offset\"),\n",
    "    \n",
    "    # 4. Properties (Alerts & Status)\n",
    "    col(\"parsed.properties.status\").alias(\"status\"),\n",
    "    col(\"parsed.properties.alert\").alias(\"alert_level\"),\n",
    "    col(\"parsed.properties.tsunami\").alias(\"tsunami_flag\"),\n",
    "    col(\"parsed.properties.sig\").alias(\"significance\"),\n",
    "    col(\"parsed.properties.felt\").alias(\"felt_reports\"),\n",
    "    col(\"parsed.properties.cdi\").alias(\"cdi_intensity\"),\n",
    "    col(\"parsed.properties.mmi\").alias(\"mmi_intensity\"),\n",
    "    \n",
    "    # 5. Technical Data\n",
    "    col(\"parsed.properties.net\").alias(\"network\"),\n",
    "    col(\"parsed.properties.code\").alias(\"source_code\"),\n",
    "    col(\"parsed.properties.nst\").alias(\"station_count\"),\n",
    "    col(\"parsed.properties.dmin\").alias(\"min_distance\"),\n",
    "    col(\"parsed.properties.rms\").alias(\"rms_error\"),\n",
    "    col(\"parsed.properties.gap\").alias(\"azimuth_gap\"),\n",
    "    \n",
    "    # 6. URLs\n",
    "    col(\"parsed.properties.url\").alias(\"usgs_url\")\n",
    ")\n",
    "\n",
    "# ==========================================\n",
    "# 5. WRITE STREAM (To Silver Container)\n",
    "# ==========================================\n",
    "query = (df_silver.writeStream\n",
    "    .format(\"delta\")\n",
    "    .outputMode(\"append\")\n",
    "    .option(\"checkpointLocation\", checkpoint_path)\n",
    "    .option(\"path\", silver_path)        # <--- Forces data to 'silver' container\n",
    "    .option(\"mergeSchema\", \"true\")      # Auto-evolve if USGS adds fields\n",
    "    .trigger(availableNow=True)\n",
    "    .table(target_table))\n",
    "\n",
    "print(f\"âœ… Stream Started! Transforming data into {target_table}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77478ddf-a433-4613-baca-a6410700d770",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT \n",
    "    event_time, \n",
    "    magnitude, \n",
    "    place, \n",
    "    latitude, \n",
    "    longitude, \n",
    "    tsunami_flag \n",
    "FROM databricks_cata.silver.silver_earthquake \n",
    "ORDER BY magnitude DESC \n",
    "LIMIT 10"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6454224429159290,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "(Clone) Silver_Layer",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
