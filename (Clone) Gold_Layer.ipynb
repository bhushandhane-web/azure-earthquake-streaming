{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b50e0a4-a463-443c-b3fb-deb544b4b10e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, window, max, avg, count, to_date, current_timestamp\n",
    "\n",
    "# ==========================================\n",
    "# 1. CONFIGURATION\n",
    "# ==========================================\n",
    "storage_account_name = \"databricksete1995\"\n",
    "gold_container       = \"gold\"\n",
    "\n",
    "# Input (Read from Silver)\n",
    "source_table = \"databricks_cata.silver.silver_earthquake\"\n",
    "\n",
    "# Outputs (Two different Gold Tables)\n",
    "target_table_daily  = \"databricks_cata.gold.gold_daily_summary\"\n",
    "target_table_impact = \"databricks_cata.gold.gold_high_impact\"\n",
    "\n",
    "# Checkpoint Paths (Must be unique for each stream)\n",
    "checkpoint_daily  = f\"abfss://{gold_container}@{storage_account_name}.dfs.core.windows.net/_checkpoints/gold_daily\"\n",
    "checkpoint_impact = f\"abfss://{gold_container}@{storage_account_name}.dfs.core.windows.net/_checkpoints/gold_impact\"\n",
    "\n",
    "# Storage Paths\n",
    "path_daily  = f\"abfss://{gold_container}@{storage_account_name}.dfs.core.windows.net/daily_summary\"\n",
    "path_impact = f\"abfss://{gold_container}@{storage_account_name}.dfs.core.windows.net/high_impact_events\"\n",
    "\n",
    "print(f\"üöÄ Gold Stream Initializing...\")\n",
    "print(f\"üì• Reading from: {source_table}\")\n",
    "\n",
    "# ==========================================\n",
    "# 2. READ STREAM (From Silver)\n",
    "# ==========================================\n",
    "df_silver = spark.readStream.table(source_table)\n",
    "\n",
    "# ==========================================\n",
    "# 3. STREAM A: HIGH IMPACT EVENTS (Filter)\n",
    "# ==========================================\n",
    "df_impact = df_silver.filter(\n",
    "    (col(\"magnitude\") >= 3.0)\n",
    ")\n",
    "\n",
    "query_impact = (df_impact.writeStream\n",
    "    .format(\"delta\")\n",
    "    .outputMode(\"append\")\n",
    "    .option(\"checkpointLocation\", checkpoint_impact)\n",
    "    .option(\"path\", path_impact)\n",
    "    .option(\"mergeSchema\", \"true\")\n",
    "    \n",
    "    # ‚úÖ 1. Process Batch & Stop\n",
    "    .trigger(availableNow=True) \n",
    "    \n",
    "    .table(target_table_impact))\n",
    "\n",
    "print(f\"‚úÖ Stream A Started: '{target_table_impact}'\")\n",
    "\n",
    "# ==========================================\n",
    "# 4. STREAM B: DAILY AGGREGATION (Aggregate)\n",
    "# ==========================================\n",
    "df_daily = (df_silver\n",
    "    .withWatermark(\"event_time\", \"24 hours\")\n",
    "    .groupBy(window(col(\"event_time\"), \"1 day\").alias(\"time_window\"))\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"total_earthquakes\"),\n",
    "        max(\"magnitude\").alias(\"max_magnitude\"),\n",
    "        avg(\"depth\").alias(\"avg_depth_km\"),\n",
    "        count(\"tsunami_flag\").alias(\"tsunami_alerts_count\")\n",
    "    )\n",
    "    .select(\n",
    "        col(\"time_window.start\").alias(\"date\"),\n",
    "        col(\"total_earthquakes\"),\n",
    "        col(\"max_magnitude\"),\n",
    "        col(\"avg_depth_km\"),\n",
    "        col(\"tsunami_alerts_count\"),\n",
    "        current_timestamp().alias(\"last_calculated_at\")\n",
    "    )\n",
    ")\n",
    "\n",
    "query_daily = (df_daily.writeStream\n",
    "    .format(\"delta\")\n",
    "    .outputMode(\"complete\")\n",
    "    .option(\"checkpointLocation\", checkpoint_daily)\n",
    "    .option(\"path\", path_daily)\n",
    "    .option(\"mergeSchema\", \"true\")\n",
    "    \n",
    "    # ‚úÖ 2. Process Batch & Stop\n",
    "    .trigger(availableNow=True)\n",
    "    \n",
    "    .table(target_table_daily))\n",
    "\n",
    "print(f\"‚úÖ Stream B Started: '{target_table_daily}'\")\n",
    "\n",
    "# ==========================================\n",
    "# 5. WAIT FOR COMPLETION (Crucial for ADF)\n",
    "# ==========================================\n",
    "print(\"‚è≥ Waiting for both streams to finish processing...\")\n",
    "\n",
    "# This forces the Notebook to pause here until BOTH tables are fully updated.\n",
    "query_impact.awaitTermination()\n",
    "query_daily.awaitTermination()\n",
    "\n",
    "print(\"üöÄ Gold Layer Processing Complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c60c9dd2-82e7-4c01-b03d-9556227ff424",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# CREATE GOLD MAP TABLE (Required for 3D Map)\n",
    "# ==========================================\n",
    "storage_account_name = \"databricksete1995\"\n",
    "gold_container       = \"gold\"\n",
    "\n",
    "# 1. Configuration\n",
    "source_table = \"databricks_cata.silver.silver_earthquake\"\n",
    "target_table = \"databricks_cata.gold.gold_live_map\"\n",
    "\n",
    "checkpoint_path = f\"abfss://{gold_container}@{storage_account_name}.dfs.core.windows.net/_checkpoints/gold_map_v2\"\n",
    "output_path     = f\"abfss://{gold_container}@{storage_account_name}.dfs.core.windows.net/gold_live_map\"\n",
    "\n",
    "# 2. Read Clean Data\n",
    "df_silver = spark.readStream.table(source_table)\n",
    "\n",
    "# 3. Select Only Map Columns (Lat/Lon/Depth are crucial here)\n",
    "df_map = df_silver.select(\n",
    "    \"event_time\",\n",
    "    \"latitude\",\n",
    "    \"longitude\",\n",
    "    \"depth\",         # <--- Crucial for 3D visualization\n",
    "    \"magnitude\",\n",
    "    \"place\",\n",
    "    \"alert_level\"\n",
    ")\n",
    "\n",
    "# 4. Write to Gold\n",
    "# We use 'trigger(availableNow=True)' to process all current data immediately and then stop.\n",
    "# This makes sure the table is ready for your 3D plot instantly.\n",
    "(df_map.writeStream\n",
    "    .format(\"delta\")\n",
    "    .outputMode(\"append\")\n",
    "    .option(\"checkpointLocation\", checkpoint_path)\n",
    "    .option(\"path\", output_path)\n",
    "    .trigger(availableNow=True) \n",
    "    .table(target_table))\n",
    "\n",
    "print(f\"‚úÖ Created table: {target_table}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "983a37eb-ad18-4945-a050-347b70024cf1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "# 1. Load the Gold Data\n",
    "df = spark.table(\"databricks_cata.gold.gold_live_map\").toPandas()\n",
    "\n",
    "# 2. Create the 3D Scatter Plot\n",
    "# X = Longitude, Y = Latitude, Z = Depth (Inverted because depth goes down!)\n",
    "fig = px.scatter_3d(df, \n",
    "                    x='longitude', \n",
    "                    y='latitude', \n",
    "                    z='depth',\n",
    "                    color='magnitude',       # Color dots by strength\n",
    "                    size='magnitude',        # Size dots by strength\n",
    "                    hover_name='place',      # Show name when hovering\n",
    "                    opacity=0.7,\n",
    "                    color_continuous_scale=px.colors.sequential.Viridis,\n",
    "                    title=\"3D Earthquake Map: Location vs Depth\")\n",
    "\n",
    "# 3. Invert Z-Axis (So deep quakes look \"deep\")\n",
    "fig.update_scenes(zaxis_autorange=\"reversed\")\n",
    "\n",
    "# 4. Display\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1977443c-7278-4e6d-b237-b5f654e59ffa",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{\"usgs_url\":{\"format\":{\"preset\":\"string-preset-url\",\"locale\":\"en\"}}}},\"syncTimestamp\":1770098705352}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(spark.read.table(\"databricks_cata.gold.gold_high_impact\"))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "(Clone) Gold_Layer",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
